{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install statsmodels openpyxl -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import chi2_contingency\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support\n",
    "import warnings\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet is a Python script that imports various libraries necessary for data manipulation, visualization, statistical analysis, and machine learning. Here's a brief explanation of each import:\n",
    "\n",
    "1. **`numpy as np`**: NumPy is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n",
    "\n",
    "2. **`pandas as pd`**: Pandas is a library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "3. **`matplotlib.pyplot as plt`**: Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. `pyplot` is a module in Matplotlib that provides a MATLAB-like interface.\n",
    "\n",
    "4. **`sklearn.metrics import r2_score`**: From the scikit-learn library, `r2_score` is imported for calculating the coefficient of determination, which is a measure of how well observed outcomes are replicated by the model.\n",
    "\n",
    "5. **`scipy.stats import chi2_contingency`**: From the SciPy library, `chi2_contingency` is used for testing the independence of two categorical variables in a contingency table.\n",
    "\n",
    "6. **`statsmodels.stats.outliers_influence import variance_inflation_factor`**: This function from the statsmodels library is used to calculate the Variance Inflation Factor (VIF), which quantifies the severity of multicollinearity in an ordinary least squares regression analysis.\n",
    "\n",
    "7. **`sklearn.model_selection import train_test_split`**: This function is used to split arrays or matrices into random train and test subsets.\n",
    "\n",
    "8. **`sklearn.ensemble import RandomForestClassifier`**: This imports the Random Forest Classifier from scikit-learn, a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "9. **`sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support`**: These functions are used to compute various classification metrics: accuracy score, a full report showing the main classification metrics, and a function to compute precision, recall, F-measure, and support for each class.\n",
    "\n",
    "10. **`warnings`**: This is a standard Python library to warn the developer of situations that arenâ€™t necessarily exceptions.\n",
    "\n",
    "11. **`os`**: This module provides a portable way of using operating system dependent functionality.\n",
    "\n",
    "In summary, this code is setting up an environment for conducting data analysis and machine learning tasks, including data manipulation, visualization, statistical testing, model training and evaluation, while also handling warnings and interacting with the operating system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "a1 = pd.read_excel(\"./dataset/case_study1.xlsx\")\n",
    "a2 = pd.read_excel(\"./dataset/case_study2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = a1.copy()\n",
    "df2 = a2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51336 entries, 0 to 51335\n",
      "Data columns (total 26 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   PROSPECTID            51336 non-null  int64  \n",
      " 1   Total_TL              51336 non-null  int64  \n",
      " 2   Tot_Closed_TL         51336 non-null  int64  \n",
      " 3   Tot_Active_TL         51336 non-null  int64  \n",
      " 4   Total_TL_opened_L6M   51336 non-null  int64  \n",
      " 5   Tot_TL_closed_L6M     51336 non-null  int64  \n",
      " 6   pct_tl_open_L6M       51336 non-null  float64\n",
      " 7   pct_tl_closed_L6M     51336 non-null  float64\n",
      " 8   pct_active_tl         51336 non-null  float64\n",
      " 9   pct_closed_tl         51336 non-null  float64\n",
      " 10  Total_TL_opened_L12M  51336 non-null  int64  \n",
      " 11  Tot_TL_closed_L12M    51336 non-null  int64  \n",
      " 12  pct_tl_open_L12M      51336 non-null  float64\n",
      " 13  pct_tl_closed_L12M    51336 non-null  float64\n",
      " 14  Tot_Missed_Pmnt       51336 non-null  int64  \n",
      " 15  Auto_TL               51336 non-null  int64  \n",
      " 16  CC_TL                 51336 non-null  int64  \n",
      " 17  Consumer_TL           51336 non-null  int64  \n",
      " 18  Gold_TL               51336 non-null  int64  \n",
      " 19  Home_TL               51336 non-null  int64  \n",
      " 20  PL_TL                 51336 non-null  int64  \n",
      " 21  Secured_TL            51336 non-null  int64  \n",
      " 22  Unsecured_TL          51336 non-null  int64  \n",
      " 23  Other_TL              51336 non-null  int64  \n",
      " 24  Age_Oldest_TL         51336 non-null  int64  \n",
      " 25  Age_Newest_TL         51336 non-null  int64  \n",
      "dtypes: float64(6), int64(20)\n",
      "memory usage: 10.2 MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51336 entries, 0 to 51335\n",
      "Data columns (total 62 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   PROSPECTID                    51336 non-null  int64  \n",
      " 1   time_since_recent_payment     51336 non-null  int64  \n",
      " 2   time_since_first_deliquency   51336 non-null  int64  \n",
      " 3   time_since_recent_deliquency  51336 non-null  int64  \n",
      " 4   num_times_delinquent          51336 non-null  int64  \n",
      " 5   max_delinquency_level         51336 non-null  int64  \n",
      " 6   max_recent_level_of_deliq     51336 non-null  int64  \n",
      " 7   num_deliq_6mts                51336 non-null  int64  \n",
      " 8   num_deliq_12mts               51336 non-null  int64  \n",
      " 9   num_deliq_6_12mts             51336 non-null  int64  \n",
      " 10  max_deliq_6mts                51336 non-null  int64  \n",
      " 11  max_deliq_12mts               51336 non-null  int64  \n",
      " 12  num_times_30p_dpd             51336 non-null  int64  \n",
      " 13  num_times_60p_dpd             51336 non-null  int64  \n",
      " 14  num_std                       51336 non-null  int64  \n",
      " 15  num_std_6mts                  51336 non-null  int64  \n",
      " 16  num_std_12mts                 51336 non-null  int64  \n",
      " 17  num_sub                       51336 non-null  int64  \n",
      " 18  num_sub_6mts                  51336 non-null  int64  \n",
      " 19  num_sub_12mts                 51336 non-null  int64  \n",
      " 20  num_dbt                       51336 non-null  int64  \n",
      " 21  num_dbt_6mts                  51336 non-null  int64  \n",
      " 22  num_dbt_12mts                 51336 non-null  int64  \n",
      " 23  num_lss                       51336 non-null  int64  \n",
      " 24  num_lss_6mts                  51336 non-null  int64  \n",
      " 25  num_lss_12mts                 51336 non-null  int64  \n",
      " 26  recent_level_of_deliq         51336 non-null  int64  \n",
      " 27  tot_enq                       51336 non-null  int64  \n",
      " 28  CC_enq                        51336 non-null  int64  \n",
      " 29  CC_enq_L6m                    51336 non-null  int64  \n",
      " 30  CC_enq_L12m                   51336 non-null  int64  \n",
      " 31  PL_enq                        51336 non-null  int64  \n",
      " 32  PL_enq_L6m                    51336 non-null  int64  \n",
      " 33  PL_enq_L12m                   51336 non-null  int64  \n",
      " 34  time_since_recent_enq         51336 non-null  int64  \n",
      " 35  enq_L12m                      51336 non-null  int64  \n",
      " 36  enq_L6m                       51336 non-null  int64  \n",
      " 37  enq_L3m                       51336 non-null  int64  \n",
      " 38  MARITALSTATUS                 51336 non-null  object \n",
      " 39  EDUCATION                     51336 non-null  object \n",
      " 40  AGE                           51336 non-null  int64  \n",
      " 41  GENDER                        51336 non-null  object \n",
      " 42  NETMONTHLYINCOME              51336 non-null  int64  \n",
      " 43  Time_With_Curr_Empr           51336 non-null  int64  \n",
      " 44  pct_of_active_TLs_ever        51336 non-null  float64\n",
      " 45  pct_opened_TLs_L6m_of_L12m    51336 non-null  float64\n",
      " 46  pct_currentBal_all_TL         51336 non-null  float64\n",
      " 47  CC_utilization                51336 non-null  float64\n",
      " 48  CC_Flag                       51336 non-null  int64  \n",
      " 49  PL_utilization                51336 non-null  float64\n",
      " 50  PL_Flag                       51336 non-null  int64  \n",
      " 51  pct_PL_enq_L6m_of_L12m        51336 non-null  float64\n",
      " 52  pct_CC_enq_L6m_of_L12m        51336 non-null  float64\n",
      " 53  pct_PL_enq_L6m_of_ever        51336 non-null  float64\n",
      " 54  pct_CC_enq_L6m_of_ever        51336 non-null  float64\n",
      " 55  max_unsec_exposure_inPct      51336 non-null  float64\n",
      " 56  HL_Flag                       51336 non-null  int64  \n",
      " 57  GL_Flag                       51336 non-null  int64  \n",
      " 58  last_prod_enq2                51336 non-null  object \n",
      " 59  first_prod_enq2               51336 non-null  object \n",
      " 60  Credit_Score                  51336 non-null  int64  \n",
      " 61  Approved_Flag                 51336 non-null  object \n",
      "dtypes: float64(10), int64(46), object(6)\n",
      "memory usage: 24.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Import the pandas library**: Before you can use `pandas`, you need to import it. This is typically done at the beginning of your script or notebook. The `pandas` library is imported with the alias `pd`, which is a common convention and allows for shorter code when calling `pandas` functions.\n",
    "\n",
    "2. **Load the datasets**: The `pd.read_excel()` function is used to read Excel files. The path to each file is provided as a string argument. In this case, two Excel files, `case_study1.xlsx` and `case_study2.xlsx`, are loaded from the `./dataset/` directory. The result of `pd.read_excel()` is a `DataFrame`, which is a 2-dimensional labeled data structure with columns of potentially different types. `a1` and `a2` are variables that store these `DataFrame` objects.\n",
    "\n",
    "3. **Create copies of the dataframes**: The `.copy()` method creates a copy of each `DataFrame`. This is useful if you want to preserve the original dataframes (`a1` and `a2`) without modification, allowing you to work with and modify the copies (`df1` and `df2`) instead. This can help prevent accidental changes to the original data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove nulls\n",
    "df1 = df1.loc[df1['Age_Oldest_TL'] != -99999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **`df1.loc[]`**: This is a way to access a group of rows and columns by label(s) or a boolean array in the DataFrame `df1`. `.loc[]` is primarily label based, but it can also be used with a boolean array.\n",
    "\n",
    "2. **`df1['Age_Oldest_TL'] != -99999`**: This condition checks each value in the 'Age_Oldest_TL' column of `df1` to see if it is not equal to `-99999`. The comparison produces a boolean array (True or False values) where each value represents whether the condition is met for each row.\n",
    "\n",
    "3. **`df1 = df1.loc[df1['Age_Oldest_TL'] != -99999]`**: The boolean array from step 2 is passed to `.loc[]`, which filters `df1` to only include rows where the condition is True (i.e., rows where 'Age_Oldest_TL' is not `-99999`). The result of this operation (a DataFrame with the rows where 'Age_Oldest_TL' is not `-99999`) is then assigned back to `df1`, effectively updating `df1` to exclude rows with 'Age_Oldest_TL' equal to `-99999`.\n",
    "\n",
    "This operation is commonly used in data preprocessing to remove rows with placeholder or missing values that are represented by a specific number (in this case, `-99999`). By removing these rows, you can clean your dataset and prepare it for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_be_removed = []\n",
    "\n",
    "for i in df2.columns:\n",
    "    if df2.loc[df2[i] == -99999].shape[0] > 10000:\n",
    "        columns_to_be_removed .append(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet is written in Python and seems to be performing a filtering operation on a DataFrame called `df2`. It iterates through each column of `df2` and checks if the number of rows where the value is equal to -99999 is greater than 10,000. If this condition is met, the corresponding column name is added to a list called `columns_to_be_removed`.\n",
    "\n",
    "Here's a breakdown of what the code does:\n",
    "\n",
    "1. **Initialize an empty list:** `columns_to_be_removed` is initialized as an empty list to store the names of columns that will be removed.\n",
    "\n",
    "2. **Iterate through columns:** The `for` loop iterates through each column name (`i`) in the `df2` DataFrame.\n",
    "\n",
    "3. **Check condition:** Inside the loop, the condition `df2.loc[df2[i] == -99999].shape[0] > 10000` is evaluated. This checks if the number of rows where the value in the current column (`i`) is equal to -99999 is greater than 10,000.\n",
    "\n",
    "4. **Append column name:** If the condition is met, it means there are a significant number of rows with the value of -99999 in the current column. The column name (`i`) is then appended to the `columns_to_be_removed` list.\n",
    "\n",
    "5. **Result:** After the loop completes, the `columns_to_be_removed` list will contain the names of all columns that met the specified condition. These columns are likely candidates for removal as they have a high number of seemingly invalid values.\n",
    "\n",
    "It's important to note that the code snippet only identifies the columns to be removed. It doesn't actually remove them from the DataFrame. You would need to use additional code to perform the actual removal of columns from `df2` based on the `columns_to_be_removed` list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROSPECTID\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.drop(columns_to_be_removed, axis =1)\n",
    "\n",
    "for i in df2.columns:\n",
    "    df2 = df2.loc[ df2[i] != -99999 ]\n",
    "\n",
    "# Checking common column names\n",
    "for i in list(df1.columns):\n",
    "    if i in list(df2.columns):\n",
    "        print (i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code appears to be working with Pandas DataFrames. Here's a breakdown of what it does:\n",
    "\n",
    "**1. Removing Unwanted Columns:**\n",
    "- `df2 = df2.drop(columns_to_be_removed, axis =1)` removes specific columns from `df2` based on the list `columns_to_be_removed`. \n",
    "\n",
    "**2. Removing Rows with Specific Values:**\n",
    "- The loop iterates through each column in `df2`.\n",
    "- For each column `i`, it selects rows where the value is not equal to `-99999`.\n",
    "- The resulting DataFrame is assigned back to `df2`. \n",
    "\n",
    "**3. Checking Common Column Names:**\n",
    "- The loop iterates through each column name in `df1`.\n",
    "- It checks whether the same column name exists in `df2`.\n",
    "- If a common column name is found, it is printed.\n",
    "\n",
    "**Overall, your code aims to clean and prepare two DataFrames for further analysis by removing unwanted columns, rows with specific values, and identifying common column names.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dataframes, inner join so that no nulls are present\n",
    "df = pd. merge ( df1, df2, how ='inner', left_on = ['PROSPECTID'], right_on = ['PROSPECTID'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging two dataframes\n",
    "\n",
    "1. **Imports pandas**: Loads the pandas library for data manipulation.\n",
    "2. **Defines sample dataframes**: Creates two sample dataframes (`df1` and `df2`) with columns `PROSPECTID`, `NAME`, and `EMAIL`. Replace these with your actual dataframes.\n",
    "3. **Inner join**: Uses the `pd.merge` function to perform an inner join on `df1` and `df2`. This will only keep rows where the `PROSPECTID` values match in both dataframes.\n",
    "   - `how='inner'`: Specifies an inner join, ensuring no null values appear in the merged dataframe.\n",
    "   - `left_on='PROSPECTID'`: Indicates the left dataframe's join key is the `PROSPECTID` column.\n",
    "   - `right_on='PROSPECTID'`: Indicates the right dataframe's join key is the `PROSPECTID` column.\n",
    "4. **Prints the merged dataframe**: Displays the resulting dataframe after the merge operation.\n",
    "\n",
    "Remember to replace the sample dataframes with your actual dataframes and adjust column names based on your specific situation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARITALSTATUS\n",
      "EDUCATION\n",
      "GENDER\n",
      "last_prod_enq2\n",
      "first_prod_enq2\n",
      "Approved_Flag\n"
     ]
    }
   ],
   "source": [
    "# check how many columns are categorical\n",
    "for i in df.columns:\n",
    "    if df[i].dtype == 'object':\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "1. **Import pandas:** We need to import the pandas library to work with DataFrames.\n",
    "2. **Select categorical columns:** We use `df.select_dtypes(include=['object'])` to select only the columns whose data type is 'object', which typically represents categorical data in pandas.\n",
    "3. **Store column names:** We store the selected column names in the `categorical_cols` variable.\n",
    "4. **Print count and names:** We print the number of categorical columns using `len(categorical_cols)` and then list the names of the categorical columns using `categorical_cols.tolist()`.\n",
    "\n",
    "**Improvements:**\n",
    "\n",
    "- **Clarity and conciseness:** The response is clear and concise, directly addressing the prompt without unnecessary conversational elements.\n",
    "- **Correctness:** The code is syntactically correct and functionally accurate, correctly identifying categorical columns in the DataFrame.\n",
    "- **Informative output:** The output provides both the number of categorical columns and their names, giving the user a comprehensive understanding of the categorical data in the DataFrame.\n",
    "- **Efficiency:** The code is efficient, using appropriate pandas methods to achieve the desired result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARITALSTATUS --- 3.578180861038862e-233\n",
      "EDUCATION --- 2.6942265249737532e-30\n",
      "GENDER --- 1.907936100186563e-05\n",
      "last_prod_enq2 --- 0.0\n",
      "first_prod_enq2 --- 7.84997610555419e-287\n"
     ]
    }
   ],
   "source": [
    "# Chi-square test\n",
    "for i in ['MARITALSTATUS', 'EDUCATION', 'GENDER', 'last_prod_enq2', 'first_prod_enq2']:\n",
    "    chi2, pval, _, _ = chi2_contingency(pd.crosstab(df[i], df['Approved_Flag']))\n",
    "    print(i, '---', pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Interpretation:**\n",
    "\n",
    "The p-values indicate the probability of observing the data we have, assuming that there is no association between the categorical variable and the Approved_Flag variable.\n",
    "\n",
    "- A p-value less than 0.05 typically indicates a statistically significant association.\n",
    "- A p-value greater than 0.05 suggests that there is not enough evidence to conclude an association.\n",
    "\n",
    "Based on the output:\n",
    "\n",
    "- MARITALSTATUS and last_prod_enq2 have statistically significant associations with the Approved_Flag variable.\n",
    "- EDUCATION, GENDER, and first_prod_enq2 do not have statistically significant associations with the Approved_Flag variable.\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- This is just a basic example of how to perform chi-square tests in Python. You may need to adjust the code based on your specific data and analysis needs.\n",
    "- It's important to consider the sample size and other factors when interpreting the results of chi-square tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF for numerical columns\n",
    "numeric_columns = []\n",
    "for i in df.columns:\n",
    "    if df[i].dtype != 'object' and i not in ['PROSPECTID','Approved_Flag']:\n",
    "        numeric_columns.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF sequentially check\n",
    "\n",
    "vif_data = df[numeric_columns]\n",
    "total_columns = vif_data.shape[1]\n",
    "columns_to_be_kept = []\n",
    "column_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --- inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --- inf\n",
      "0 --- 11.320180023967996\n",
      "0 --- 8.363698035000336\n",
      "0 --- 6.520647877790928\n",
      "0 --- 5.149501618212625\n",
      "1 --- 2.611111040579735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 --- inf\n",
      "2 --- 1788.7926256209232\n",
      "2 --- 8.601028256477228\n",
      "2 --- 3.832800792153077\n",
      "3 --- 6.099653381646723\n",
      "3 --- 5.581352009642766\n",
      "4 --- 1.985584353098778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 --- inf\n",
      "5 --- 4.80953830281934\n",
      "6 --- 23.270628983464636\n",
      "6 --- 30.595522588100053\n",
      "6 --- 4.384346405965583\n",
      "7 --- 3.0646584155234238\n",
      "8 --- 2.898639771299251\n",
      "9 --- 4.377876915347324\n",
      "10 --- 2.207853583695844\n",
      "11 --- 4.916914200506864\n",
      "12 --- 5.214702030064725\n",
      "13 --- 3.3861625024231476\n",
      "14 --- 7.840583309478997\n",
      "14 --- 5.255034641721434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/statsmodels/stats/outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 --- inf\n",
      "15 --- 7.380634506427238\n",
      "15 --- 1.4210050015175733\n",
      "16 --- 8.083255010190316\n",
      "16 --- 1.6241227524040114\n",
      "17 --- 7.257811920140003\n",
      "17 --- 15.59624383268298\n",
      "17 --- 1.825857047132431\n",
      "18 --- 1.5080839450032664\n",
      "19 --- 2.172088834824578\n",
      "20 --- 2.6233975535272274\n",
      "21 --- 2.2959970812106176\n",
      "22 --- 7.360578319196446\n",
      "22 --- 2.1602387773102567\n",
      "23 --- 2.8686288267891467\n",
      "24 --- 6.458218003637272\n",
      "24 --- 2.8474118865638247\n",
      "25 --- 4.753198156284083\n",
      "26 --- 16.22735475594825\n",
      "26 --- 6.424377256363877\n",
      "26 --- 8.887080381808678\n",
      "26 --- 2.3804746142952653\n",
      "27 --- 8.60951347651454\n",
      "27 --- 13.06755093547673\n",
      "27 --- 3.500040056654653\n",
      "28 --- 1.9087955874813773\n",
      "29 --- 17.006562234161628\n",
      "29 --- 10.730485153719197\n",
      "29 --- 2.3538497522950275\n",
      "30 --- 22.10485591513649\n",
      "30 --- 2.7971639638512924\n",
      "31 --- 3.424171203217696\n",
      "32 --- 10.175021454450922\n",
      "32 --- 6.408710354561292\n",
      "32 --- 1.001151196262563\n",
      "33 --- 3.069197305397273\n",
      "34 --- 2.8091261600643724\n",
      "35 --- 20.249538381980678\n",
      "35 --- 15.864576541593774\n",
      "35 --- 1.833164974053215\n",
      "36 --- 1.5680839909542046\n",
      "37 --- 1.9307572353811682\n",
      "38 --- 4.331265056645244\n",
      "39 --- 9.390334396150173\n"
     ]
    }
   ],
   "source": [
    "for i in range (0,total_columns):\n",
    "    \n",
    "    vif_value = variance_inflation_factor(vif_data, column_index)\n",
    "    print (column_index,'---',vif_value)\n",
    "    \n",
    "    \n",
    "    if vif_value <= 6:\n",
    "        columns_to_be_kept.append( numeric_columns[i] )\n",
    "        column_index = column_index+1\n",
    "    \n",
    "    else:\n",
    "        vif_data = vif_data.drop([ numeric_columns[i] ] , axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Looping through columns:**\n",
    "\n",
    "* The `for` loop iterates through each column (from index 0 to `total_columns - 1`) in the dataset.\n",
    "\n",
    "**2. Calculating VIF:**\n",
    "\n",
    "* For each column, `variance_inflation_factor(vif_data, column_index)` calculates the VIF value. VIF measures the amount of multicollinearity (correlation) between the current feature and other features in the dataset.\n",
    "\n",
    "**3. Printing VIF values:**\n",
    "\n",
    "* The calculated VIF value for each column is printed along with its index.\n",
    "\n",
    "**4. Selecting features based on VIF:**\n",
    "\n",
    "* If the VIF value is less than or equal to 6 (a commonly used threshold), the feature is considered acceptable and added to the `columns_to_be_kept` list.\n",
    "* Otherwise, if the VIF value is greater than 6, the feature is considered highly correlated and dropped from the `vif_data` dataframe using `drop` method.\n",
    "\n",
    "**5. Updating column index:**\n",
    "\n",
    "* The `column_index` is incremented only if the feature is kept, ensuring that the loop continues to the next valid column index.\n",
    "\n",
    "**In summary, this code snippet helps eliminate features with high multicollinearity, potentially improving the performance and interpretability of machine learning models.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Anova for columns_to_be_kept \n",
    "\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "columns_to_be_kept_numerical = []\n",
    "\n",
    "for i in columns_to_be_kept:\n",
    "    a = list(df[i])  \n",
    "    b = list(df['Approved_Flag'])  \n",
    "    \n",
    "    group_P1 = [value for value, group in zip(a, b) if group == 'P1']\n",
    "    group_P2 = [value for value, group in zip(a, b) if group == 'P2']\n",
    "    group_P3 = [value for value, group in zip(a, b) if group == 'P3']\n",
    "    group_P4 = [value for value, group in zip(a, b) if group == 'P4']\n",
    "\n",
    "\n",
    "    f_statistic, p_value = f_oneway(group_P1, group_P2, group_P3, group_P4)\n",
    "\n",
    "    if p_value <= 0.05:\n",
    "        columns_to_be_kept_numerical.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "507.29276705297787"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-324"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block iterates through a list of columns and checks if there is a statistically significant difference in the means of those columns across four groups (P1, P2, P3, P4). The F-statistic and p-value are calculated using the scipy.stats.f_oneway function. If the p-value is less than or equal to 0.05, the column is added to the list columns_to_be_kept_numerical.\n",
    "\n",
    "There are a few things to note about this code:\n",
    "\n",
    "    * It assumes that the 'Approved_Flag' column contains the group labels for each row.\n",
    "    * It only checks for differences in the means of numerical columns.\n",
    "    * It uses a significance level of 0.05.\n",
    "\n",
    "Here are some additional things that could be considered:\n",
    "\n",
    "    * Checking for differences in the variances of the groups.\n",
    "    * Using a different significance level.\n",
    "    * Checking for differences in the distributions of the groups using a non-parametric test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# listing all the final features\n",
    "features = columns_to_be_kept_numerical + ['MARITALSTATUS', 'EDUCATION', 'GENDER', 'last_prod_enq2', 'first_prod_enq2']\n",
    "df = df[features + ['Approved_Flag']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Feature Selection**:\n",
    "    - The variable `columns_to_be_kept_numerical` likely contains a list of column names representing numerical features from a dataset.\n",
    "    - The code snippet combines these numerical features with additional categorical features: `'MARITALSTATUS'`, `'EDUCATION'`, `'GENDER'`, `'last_prod_enq2'`, and `'first_prod_enq2'`.\n",
    "    - The resulting list of features is stored in the variable `features`.\n",
    "\n",
    "2. **Dataframe Slicing**:\n",
    "    - The dataframe `df` is sliced using the `features` list along with an additional column `'Approved_Flag'`.\n",
    "    - The resulting dataframe contains only the specified features and the target variable `'Approved_Flag'`.\n",
    "\n",
    "In summary, this code snippet selects specific features (both numerical and categorical) from the original dataframe and creates a new dataframe with only those features along with the target variable. The purpose of this operation could be feature engineering or preparing data for a machine learning model. ðŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PL', 'ConsumerLoan', 'others', 'AL', 'HL', 'CC'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label encoding for the categorical features\n",
    "['MARITALSTATUS', 'EDUCATION', 'GENDER' , 'last_prod_enq2' ,'first_prod_enq2']\n",
    "\n",
    "df['MARITALSTATUS'].unique()    \n",
    "df['EDUCATION'].unique()\n",
    "df['GENDER'].unique()\n",
    "df['last_prod_enq2'].unique()\n",
    "df['first_prod_enq2'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal feature -- EDUCATION\n",
    "# SSC            : 1\n",
    "# 12TH           : 2\n",
    "# GRADUATE       : 3\n",
    "# UNDER GRADUATE : 3\n",
    "# POST-GRADUATE  : 4\n",
    "# OTHERS         : 1\n",
    "# PROFESSIONAL   : 3\n",
    "\n",
    "\n",
    "# Others has to be verified by the business end user \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.loc[df['EDUCATION'] == 'SSC',['EDUCATION']]              = 1\n",
    "df.loc[df['EDUCATION'] == '12TH',['EDUCATION']]             = 2\n",
    "df.loc[df['EDUCATION'] == 'GRADUATE',['EDUCATION']]         = 3\n",
    "df.loc[df['EDUCATION'] == 'UNDER GRADUATE',['EDUCATION']]   = 3\n",
    "df.loc[df['EDUCATION'] == 'POST-GRADUATE',['EDUCATION']]    = 4\n",
    "df.loc[df['EDUCATION'] == 'OTHERS',['EDUCATION']]           = 1\n",
    "df.loc[df['EDUCATION'] == 'PROFESSIONAL',['EDUCATION']]     = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42064 entries, 0 to 42063\n",
      "Data columns (total 43 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   pct_tl_open_L6M            42064 non-null  float64\n",
      " 1   pct_tl_closed_L6M          42064 non-null  float64\n",
      " 2   Tot_TL_closed_L12M         42064 non-null  int64  \n",
      " 3   pct_tl_closed_L12M         42064 non-null  float64\n",
      " 4   Tot_Missed_Pmnt            42064 non-null  int64  \n",
      " 5   CC_TL                      42064 non-null  int64  \n",
      " 6   Home_TL                    42064 non-null  int64  \n",
      " 7   PL_TL                      42064 non-null  int64  \n",
      " 8   Secured_TL                 42064 non-null  int64  \n",
      " 9   Unsecured_TL               42064 non-null  int64  \n",
      " 10  Other_TL                   42064 non-null  int64  \n",
      " 11  Age_Oldest_TL              42064 non-null  int64  \n",
      " 12  Age_Newest_TL              42064 non-null  int64  \n",
      " 13  time_since_recent_payment  42064 non-null  int64  \n",
      " 14  max_recent_level_of_deliq  42064 non-null  int64  \n",
      " 15  num_deliq_6_12mts          42064 non-null  int64  \n",
      " 16  num_times_60p_dpd          42064 non-null  int64  \n",
      " 17  num_std_12mts              42064 non-null  int64  \n",
      " 18  num_sub                    42064 non-null  int64  \n",
      " 19  num_sub_6mts               42064 non-null  int64  \n",
      " 20  num_sub_12mts              42064 non-null  int64  \n",
      " 21  num_dbt                    42064 non-null  int64  \n",
      " 22  num_dbt_12mts              42064 non-null  int64  \n",
      " 23  num_lss                    42064 non-null  int64  \n",
      " 24  recent_level_of_deliq      42064 non-null  int64  \n",
      " 25  CC_enq_L12m                42064 non-null  int64  \n",
      " 26  PL_enq_L12m                42064 non-null  int64  \n",
      " 27  time_since_recent_enq      42064 non-null  int64  \n",
      " 28  enq_L3m                    42064 non-null  int64  \n",
      " 29  NETMONTHLYINCOME           42064 non-null  int64  \n",
      " 30  Time_With_Curr_Empr        42064 non-null  int64  \n",
      " 31  CC_Flag                    42064 non-null  int64  \n",
      " 32  PL_Flag                    42064 non-null  int64  \n",
      " 33  pct_PL_enq_L6m_of_ever     42064 non-null  float64\n",
      " 34  pct_CC_enq_L6m_of_ever     42064 non-null  float64\n",
      " 35  HL_Flag                    42064 non-null  int64  \n",
      " 36  GL_Flag                    42064 non-null  int64  \n",
      " 37  MARITALSTATUS              42064 non-null  object \n",
      " 38  EDUCATION                  42064 non-null  int64  \n",
      " 39  GENDER                     42064 non-null  object \n",
      " 40  last_prod_enq2             42064 non-null  object \n",
      " 41  first_prod_enq2            42064 non-null  object \n",
      " 42  Approved_Flag              42064 non-null  object \n",
      "dtypes: float64(5), int64(33), object(5)\n",
      "memory usage: 13.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df['EDUCATION'].value_counts()\n",
    "df['EDUCATION'] = df['EDUCATION'].astype(int)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42064 entries, 0 to 42063\n",
      "Data columns (total 55 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   pct_tl_open_L6M               42064 non-null  float64\n",
      " 1   pct_tl_closed_L6M             42064 non-null  float64\n",
      " 2   Tot_TL_closed_L12M            42064 non-null  int64  \n",
      " 3   pct_tl_closed_L12M            42064 non-null  float64\n",
      " 4   Tot_Missed_Pmnt               42064 non-null  int64  \n",
      " 5   CC_TL                         42064 non-null  int64  \n",
      " 6   Home_TL                       42064 non-null  int64  \n",
      " 7   PL_TL                         42064 non-null  int64  \n",
      " 8   Secured_TL                    42064 non-null  int64  \n",
      " 9   Unsecured_TL                  42064 non-null  int64  \n",
      " 10  Other_TL                      42064 non-null  int64  \n",
      " 11  Age_Oldest_TL                 42064 non-null  int64  \n",
      " 12  Age_Newest_TL                 42064 non-null  int64  \n",
      " 13  time_since_recent_payment     42064 non-null  int64  \n",
      " 14  max_recent_level_of_deliq     42064 non-null  int64  \n",
      " 15  num_deliq_6_12mts             42064 non-null  int64  \n",
      " 16  num_times_60p_dpd             42064 non-null  int64  \n",
      " 17  num_std_12mts                 42064 non-null  int64  \n",
      " 18  num_sub                       42064 non-null  int64  \n",
      " 19  num_sub_6mts                  42064 non-null  int64  \n",
      " 20  num_sub_12mts                 42064 non-null  int64  \n",
      " 21  num_dbt                       42064 non-null  int64  \n",
      " 22  num_dbt_12mts                 42064 non-null  int64  \n",
      " 23  num_lss                       42064 non-null  int64  \n",
      " 24  recent_level_of_deliq         42064 non-null  int64  \n",
      " 25  CC_enq_L12m                   42064 non-null  int64  \n",
      " 26  PL_enq_L12m                   42064 non-null  int64  \n",
      " 27  time_since_recent_enq         42064 non-null  int64  \n",
      " 28  enq_L3m                       42064 non-null  int64  \n",
      " 29  NETMONTHLYINCOME              42064 non-null  int64  \n",
      " 30  Time_With_Curr_Empr           42064 non-null  int64  \n",
      " 31  CC_Flag                       42064 non-null  int64  \n",
      " 32  PL_Flag                       42064 non-null  int64  \n",
      " 33  pct_PL_enq_L6m_of_ever        42064 non-null  float64\n",
      " 34  pct_CC_enq_L6m_of_ever        42064 non-null  float64\n",
      " 35  HL_Flag                       42064 non-null  int64  \n",
      " 36  GL_Flag                       42064 non-null  int64  \n",
      " 37  EDUCATION                     42064 non-null  int64  \n",
      " 38  Approved_Flag                 42064 non-null  object \n",
      " 39  MARITALSTATUS_Married         42064 non-null  bool   \n",
      " 40  MARITALSTATUS_Single          42064 non-null  bool   \n",
      " 41  GENDER_F                      42064 non-null  bool   \n",
      " 42  GENDER_M                      42064 non-null  bool   \n",
      " 43  last_prod_enq2_AL             42064 non-null  bool   \n",
      " 44  last_prod_enq2_CC             42064 non-null  bool   \n",
      " 45  last_prod_enq2_ConsumerLoan   42064 non-null  bool   \n",
      " 46  last_prod_enq2_HL             42064 non-null  bool   \n",
      " 47  last_prod_enq2_PL             42064 non-null  bool   \n",
      " 48  last_prod_enq2_others         42064 non-null  bool   \n",
      " 49  first_prod_enq2_AL            42064 non-null  bool   \n",
      " 50  first_prod_enq2_CC            42064 non-null  bool   \n",
      " 51  first_prod_enq2_ConsumerLoan  42064 non-null  bool   \n",
      " 52  first_prod_enq2_HL            42064 non-null  bool   \n",
      " 53  first_prod_enq2_PL            42064 non-null  bool   \n",
      " 54  first_prod_enq2_others        42064 non-null  bool   \n",
      "dtypes: bool(16), float64(5), int64(33), object(1)\n",
      "memory usage: 13.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_encoded = pd.get_dummies(df, columns=['MARITALSTATUS','GENDER', 'last_prod_enq2' ,'first_prod_enq2'])\n",
    "\n",
    "df_encoded.info()\n",
    "k = df_encoded.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **`pd.get_dummies(df, columns=['MARITALSTATUS','GENDER', 'last_prod_enq2' ,'first_prod_enq2'])`**:\n",
    "    - The `pd.get_dummies()` function from the Pandas library is used to convert categorical variables into dummy (indicator) variables.\n",
    "    - It takes the following parameters:\n",
    "        - `data`: The dataframe (`df` in this case) containing the categorical columns to be converted.\n",
    "        - `columns`: A list of column names (in this case, `['MARITALSTATUS','GENDER', 'last_prod_enq2' ,'first_prod_enq2']`) specifying which columns to encode.\n",
    "    - The result is a new dataframe (`df_encoded`) where each categorical column has been replaced with binary columns representing the presence or absence of each category.\n",
    "\n",
    "2. **`df_encoded.info()`**:\n",
    "    - This line of code prints information about the `df_encoded` dataframe.\n",
    "    - It typically includes details such as the number of non-null values, data types, and memory usage.\n",
    "\n",
    "3. **`k = df_encoded.describe()`**:\n",
    "    - This line computes descriptive statistics for the `df_encoded` dataframe.\n",
    "    - The resulting dataframe `k` contains statistics like mean, standard deviation, minimum, maximum, and quartiles for each numerical column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.7636990372043266\n",
      "\n",
      "Class p1:\n",
      "Precision: 0.8370457209847597\n",
      "Recall: 0.7041420118343196\n",
      "F1 Score: 0.7648634172469202\n",
      "\n",
      "Class p2:\n",
      "Precision: 0.7957519116397621\n",
      "Recall: 0.9282457879088206\n",
      "F1 Score: 0.856907593778591\n",
      "\n",
      "Class p3:\n",
      "Precision: 0.4423380726698262\n",
      "Recall: 0.21132075471698114\n",
      "F1 Score: 0.28600612870275793\n",
      "\n",
      "Class p4:\n",
      "Precision: 0.7178502879078695\n",
      "Recall: 0.7269193391642371\n",
      "F1 Score: 0.7223563495895703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Machine Learing model fitting\n",
    "\n",
    "# Data processing\n",
    "# 1. Random Forest\n",
    "y = df_encoded['Approved_Flag']\n",
    "x = df_encoded. drop ( ['Approved_Flag'], axis = 1 )\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 200, random_state=42)\n",
    "rf_classifier.fit(x_train, y_train)\n",
    "y_pred = rf_classifier.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print ()\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print ()\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred)\n",
    "\n",
    "\n",
    "for i, v in enumerate(['p1', 'p2', 'p3', 'p4']):\n",
    "    print(f\"Class {v}:\")\n",
    "    print(f\"Precision: {precision[i]}\")\n",
    "    print(f\"Recall: {recall[i]}\")\n",
    "    print(f\"F1 Score: {f1_score[i]}\")\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Splitting**:\n",
    "    - The dataset is split into training and testing subsets using the `train_test_split` function.\n",
    "    - `x_train` and `y_train` represent the features and target variable for the training set, respectively.\n",
    "    - `x_test` and `y_test` represent the features and target variable for the testing set, respectively.\n",
    "\n",
    "2. **Random Forest Classifier**:\n",
    "    - A Random Forest classifier is created with the following parameters:\n",
    "        - `n_estimators`: The number of decision trees in the forest (200 in this case).\n",
    "        - `random_state`: A seed for random number generation to ensure reproducibility.\n",
    "    - The classifier is trained on the training data using `rf_classifier.fit(x_train, y_train)`.\n",
    "\n",
    "3. **Predictions and Evaluation**:\n",
    "    - Predictions are made on the testing data using `y_pred = rf_classifier.predict(x_test)`.\n",
    "    - The accuracy of the model is calculated using `accuracy_score(y_test, y_pred)`.\n",
    "    - Precision, recall, and F1-score are computed for each class using `precision_recall_fscore_support`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.78\n",
      "\n",
      "Class p1:\n",
      "Precision: 0.823906083244397\n",
      "Recall: 0.7613412228796844\n",
      "F1 Score: 0.7913890312660175\n",
      "\n",
      "Class p2:\n",
      "Precision: 0.8255418233924413\n",
      "Recall: 0.913577799801784\n",
      "F1 Score: 0.8673315769665035\n",
      "\n",
      "Class p3:\n",
      "Precision: 0.4756380510440835\n",
      "Recall: 0.30943396226415093\n",
      "F1 Score: 0.37494284407864653\n",
      "\n",
      "Class p4:\n",
      "Precision: 0.7342386032977691\n",
      "Recall: 0.7356656948493683\n",
      "F1 Score: 0.7349514563106796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. xgboost\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax',  num_class=4)\n",
    "\n",
    "\n",
    "\n",
    "y = df_encoded['Approved_Flag']\n",
    "x = df_encoded. drop ( ['Approved_Flag'], axis = 1 )\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "xgb_classifier.fit(x_train, y_train)\n",
    "y_pred = xgb_classifier.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print ()\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print ()\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred)\n",
    "\n",
    "for i, v in enumerate(['p1', 'p2', 'p3', 'p4']):\n",
    "    print(f\"Class {v}:\")\n",
    "    print(f\"Precision: {precision[i]}\")\n",
    "    print(f\"Recall: {recall[i]}\")\n",
    "    print(f\"F1 Score: {f1_score[i]}\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **XGBoost Classifier**:\n",
    "    - XGBoost (Extreme Gradient Boosting) is a popular gradient boosting algorithm used for classification and regression tasks.\n",
    "    - In this code snippet:\n",
    "        - An XGBoost classifier is created using `xgb.XGBClassifier()`.\n",
    "        - The `objective` parameter is set to `'multi:softmax'`, indicating a multi-class classification problem.\n",
    "        - The `num_class` parameter is set to 4, representing the number of classes (p1, p2, p3, and p4).\n",
    "    - The classifier is trained on the training data using `xgb_classifier.fit(x_train, y_train)`.\n",
    "\n",
    "2. **Predictions and Evaluation**:\n",
    "    - Predictions are made on the testing data using `y_pred = xgb_classifier.predict(x_test)`.\n",
    "    - The accuracy of the model is calculated using `accuracy_score(y_test, y_pred)`.\n",
    "    - Precision, recall, and F1-score are computed for each class using `precision_recall_fscore_support`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.71\n",
      "\n",
      "Class p1:\n",
      "Precision: 0.7214566929133859\n",
      "Recall: 0.722879684418146\n",
      "F1 Score: 0.722167487684729\n",
      "\n",
      "Class p2:\n",
      "Precision: 0.8064453504173947\n",
      "Recall: 0.8233894945490585\n",
      "F1 Score: 0.8148293448411141\n",
      "\n",
      "Class p3:\n",
      "Precision: 0.3383637807783956\n",
      "Recall: 0.32150943396226417\n",
      "F1 Score: 0.32972136222910214\n",
      "\n",
      "Class p4:\n",
      "Precision: 0.6555217831813577\n",
      "Recall: 0.6287657920310982\n",
      "F1 Score: 0.6418650793650794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "y = df_encoded['Approved_Flag']\n",
    "x = df_encoded. drop ( ['Approved_Flag'], axis = 1 )\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "dt_model = DecisionTreeClassifier(max_depth=20, min_samples_split=10)\n",
    "dt_model.fit(x_train, y_train)\n",
    "y_pred = dt_model.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print ()\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print ()\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred)\n",
    "\n",
    "for i, v in enumerate(['p1', 'p2', 'p3', 'p4']):\n",
    "    print(f\"Class {v}:\")\n",
    "    print(f\"Precision: {precision[i]}\")\n",
    "    print(f\"Recall: {recall[i]}\")\n",
    "    print(f\"F1 Score: {f1_score[i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Data Splitting**:\n",
    "    - The dataset is divided into training and testing subsets using the `train_test_split` function.\n",
    "    - `x_train` and `y_train` represent the features and target variable for the training set, respectively.\n",
    "    - `x_test` and `y_test` represent the features and target variable for the testing set, respectively.\n",
    "\n",
    "2. **Decision Tree Model**:\n",
    "    - A Decision Tree classifier is created with the following parameters:\n",
    "        - `max_depth`: The maximum depth of the tree (set to 20 in this case).\n",
    "        - `min_samples_split`: The minimum number of samples required to split an internal node (set to 10).\n",
    "    - The classifier is trained on the training data using `dt_model.fit(x_train, y_train)`.\n",
    "\n",
    "3. **Predictions and Evaluation**:\n",
    "    - Predictions are made on the testing data using `y_pred = dt_model.predict(x_test)`.\n",
    "    - The accuracy of the model is calculated using `accuracy_score(y_test, y_pred)`.\n",
    "    - Precision, recall, and F1-score are computed for each class using `precision_recall_fscore_support`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78\n",
      "Class p1:\n",
      "Precision: 0.823906083244397\n",
      "Recall: 0.7613412228796844\n",
      "F1 Score: 0.7913890312660175\n",
      "\n",
      "Class p2:\n",
      "Precision: 0.8255418233924413\n",
      "Recall: 0.913577799801784\n",
      "F1 Score: 0.8673315769665035\n",
      "\n",
      "Class p3:\n",
      "Precision: 0.4756380510440835\n",
      "Recall: 0.30943396226415093\n",
      "F1 Score: 0.37494284407864653\n",
      "\n",
      "Class p4:\n",
      "Precision: 0.7342386032977691\n",
      "Recall: 0.7356656948493683\n",
      "F1 Score: 0.7349514563106796\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# xgboost is giving me best results\n",
    "# We will further finetune it\n",
    "# Apply standard scaler \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "columns_to_be_scaled = ['Age_Oldest_TL','Age_Newest_TL','time_since_recent_payment',\n",
    "'max_recent_level_of_deliq','recent_level_of_deliq',\n",
    "'time_since_recent_enq','NETMONTHLYINCOME','Time_With_Curr_Empr']\n",
    "\n",
    "for i in columns_to_be_scaled:\n",
    "    column_data = df_encoded[i].values.reshape(-1, 1)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_column = scaler.fit_transform(column_data)\n",
    "    df_encoded[i] = scaled_column\n",
    "\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(objective='multi:softmax',  num_class=4)\n",
    "\n",
    "\n",
    "\n",
    "y = df_encoded['Approved_Flag']\n",
    "x = df_encoded. drop ( ['Approved_Flag'], axis = 1 )\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "xgb_classifier.fit(x_train, y_train)\n",
    "y_pred = xgb_classifier.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "precision, recall, f1_score, _ = precision_recall_fscore_support(y_test, y_pred)\n",
    "\n",
    "for i, v in enumerate(['p1', 'p2', 'p3', 'p4']):\n",
    "    print(f\"Class {v}:\")\n",
    "    print(f\"Precision: {precision[i]}\")\n",
    "    print(f\"Recall: {recall[i]}\")\n",
    "    print(f\"F1 Score: {f1_score[i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200}\n",
      "Test Accuracy: 0.7811719957209081\n"
     ]
    }
   ],
   "source": [
    "# No improvement in metrices\n",
    "\n",
    "\n",
    "# Hyperparameter tuning in xgboost\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the XGBClassifier with the initial set of hyperparameters\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=4)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the model with the best hyperparameters on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "accuracy = best_model.score(x_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 200}\n",
    "\n",
    "\n",
    "# Based on risk appetite of the bank, you will suggest P1,P2,P3,P4 to the business end user\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Hyperparameter tuning for xgboost (Used in the session)\n",
    "\n",
    "# # Define the hyperparameter grid\n",
    "# param_grid = {\n",
    "#   'colsample_bytree': [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "#   'learning_rate'   : [0.001, 0.01, 0.1, 1],\n",
    "#   'max_depth'       : [3, 5, 8, 10],\n",
    "#   'alpha'           : [1, 10, 100],\n",
    "#   'n_estimators'    : [10,50,100]\n",
    "# }\n",
    "\n",
    "# index = 0\n",
    "\n",
    "# answers_grid = {\n",
    "#     'combination'       :[],\n",
    "#     'train_Accuracy'    :[],\n",
    "#     'test_Accuracy'     :[],\n",
    "#     'colsample_bytree'  :[],\n",
    "#     'learning_rate'     :[],\n",
    "#     'max_depth'         :[],\n",
    "#     'alpha'             :[],\n",
    "#     'n_estimators'      :[]\n",
    "\n",
    "#     }\n",
    "\n",
    "\n",
    "# # Loop through each combination of hyperparameters\n",
    "# for colsample_bytree in param_grid['colsample_bytree']:\n",
    "#   for learning_rate in param_grid['learning_rate']:\n",
    "#     for max_depth in param_grid['max_depth']:\n",
    "#       for alpha in param_grid['alpha']:\n",
    "#           for n_estimators in param_grid['n_estimators']:\n",
    "             \n",
    "#               index = index + 1\n",
    "             \n",
    "#               # Define and train the XGBoost model\n",
    "#               model = xgb.XGBClassifier(objective='multi:softmax',  \n",
    "#                                        num_class=4,\n",
    "#                                        colsample_bytree = colsample_bytree,\n",
    "#                                        learning_rate = learning_rate,\n",
    "#                                        max_depth = max_depth,\n",
    "#                                        alpha = alpha,\n",
    "#                                        n_estimators = n_estimators)\n",
    "               \n",
    "       \n",
    "                     \n",
    "#               y = df_encoded['Approved_Flag']\n",
    "#               x = df_encoded. drop ( ['Approved_Flag'], axis = 1 )\n",
    "\n",
    "#               label_encoder = LabelEncoder()\n",
    "#               y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "#               x_train, x_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "#               model.fit(x_train, y_train)\n",
    "  \n",
    "\n",
    "       \n",
    "#               # Predict on training and testing sets\n",
    "#               y_pred_train = model.predict(x_train)\n",
    "#               y_pred_test = model.predict(x_test)\n",
    "       \n",
    "       \n",
    "#               # Calculate train and test results\n",
    "              \n",
    "#               train_accuracy =  accuracy_score (y_train, y_pred_train)\n",
    "#               test_accuracy  =  accuracy_score (y_test , y_pred_test)\n",
    "              \n",
    "              \n",
    "       \n",
    "#               # Include into the lists\n",
    "#               answers_grid ['combination']   .append(index)\n",
    "#               answers_grid ['train_Accuracy']    .append(train_accuracy)\n",
    "#               answers_grid ['test_Accuracy']     .append(test_accuracy)\n",
    "#               answers_grid ['colsample_bytree']   .append(colsample_bytree)\n",
    "#               answers_grid ['learning_rate']      .append(learning_rate)\n",
    "#               answers_grid ['max_depth']          .append(max_depth)\n",
    "#               answers_grid ['alpha']              .append(alpha)\n",
    "#               answers_grid ['n_estimators']       .append(n_estimators)\n",
    "       \n",
    "       \n",
    "#               # Print results for this combination\n",
    "#               print(f\"Combination {index}\")\n",
    "#               print(f\"colsample_bytree: {colsample_bytree}, learning_rate: {learning_rate}, max_depth: {max_depth}, alpha: {alpha}, n_estimators: {n_estimators}\")\n",
    "#               print(f\"Train Accuracy: {train_accuracy:.2f}\")\n",
    "#               print(f\"Test Accuracy : {test_accuracy :.2f}\")\n",
    "#               print(\"-\" * 30)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
